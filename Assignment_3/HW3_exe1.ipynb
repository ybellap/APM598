{"cells":[{"cell_type":"markdown","source":["# Team Members\n","\n","- Kshitij Dahal - 1228236635\n","- Yash Rahul Bellap - 1228575454\n","- Atharva Gupta - 1222682656\n","- Rahul Pushparajan - 1226951910\n"],"metadata":{"id":"9zj_JtETWbeJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTJKns_ct_pU","outputId":"bc057f03-08c4-4dc0-8865-2025da6b5275","executionInfo":{"status":"ok","timestamp":1712595522681,"user_tz":420,"elapsed":2104,"user":{"displayName":"Yash Bellap","userId":"09208374579413899324"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":9}],"source":["import nltk\n","from nltk.tokenize import RegexpTokenizer\n","from itertools import islice\n","from nltk.util import ngrams\n","import collections\n","from decimal import Decimal\n","import operator\n","import itertools\n","\n","\n","# Download NLTK tokenizer data\n","nltk.download('punkt')\n"]},{"cell_type":"markdown","metadata":{"id":"x2W8VdWHt_pW"},"source":["## Exercise 1\n","\n","Note: We are only tokenizing alphanumeric words and ignoring all punctuations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-GuyFNZt_pX","outputId":"6706740b-a935-4da9-8884-19919063151e","executionInfo":{"status":"ok","timestamp":1712595873594,"user_tz":420,"elapsed":632,"user":{"displayName":"Yash Bellap","userId":"09208374579413899324"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Exercise1 part1a - Count of words and unique words\n","Total Words  119160\n","Unique Words  7333\n","\n","\n","Exercise1 part1b - Top 5 words with length of 8 \n","[('certainly', 235), ('knowledge', 152), ('injustice', 111), ('therefore', 109), ('question', 99)]\n","\n","\n","Exercise1 part1c\n","Built bigram... \n","[(('the', 'republic'), 2), (('republic', 'persons'), 1), (('persons', 'of'), 3)] .... etc\n","\n","Sorting the bigram freq table to observe the most occurancies\n","\n","Top 3 most frequent bigrams:\n","[(('of', 'the'), 979), (('to', 'be'), 618), (('in', 'the'), 526)] .... etc\n","\n","\n","Observing the probability of the pair 'in the' (as one of the top 3 occurance list), where the word 'in' occurs 2157\n","0.24385720908669448\n","\n","\n","Exercise1 part1d\n","Bigram  preplexity:  41.23128859784521087914903489\n"]}],"source":["\n","\n","\n","#-------------------------- Part 1(a) ------------------------------------------------------------------------------#\n","\n","#Read the given word\n","words = open(\"/content/data_HW3_Plato_Republic.txt\", 'r').read()\n"," #Convert it to lower case\n","words = words.lower()\n","#Tokenizer which only tokenizes alpha numeric elements\n","tokenizer = RegexpTokenizer(r'\\w+')\n","tokens = tokenizer.tokenize(words)\n","\n","def part1a():\n","    '''Counts of words and unique words '''\n","    print(\"Total Words \", len(tokens))\n","    unique_word_count = len(set(tokens))\n","    print(\"Unique Words \", unique_word_count)\n","\n","print(\"\\n\\nExercise1 part1a - Count of words and unique words\")\n","part1a()\n","\n","\n","\n","#-------------------------- Part 1(b) ------------------------------------------------------------------------------#\n","\n","\n","\n","def part1b(min_length=8, top_words=5):\n","    '''Prints top 5 highest frequency words with length of at least 8'''\n","\n","    #frequency table of all worlds of all lengths\n","    freq_table = nltk.ConditionalFreqDist((len(word), word) for word in tokens)\n","\n","\n","    # frequency table of all words of min length\n","    partial_freq_table = dict()\n","    for i in range(min_length,len(freq_table)+1):\n","        partial_freq_table.update(freq_table[i])\n","\n","    #sort in decending order of frequency\n","    sorted_partial_list = sorted(partial_freq_table.items(),key=operator.itemgetter(1),reverse=True)\n","\n","    #pick top words\n","    print(list(islice(sorted_partial_list,top_words)))\n","\n","print(\"\\n\\nExercise1 part1b - Top 5 words with length of 8 \")\n","part1b(min_length=8, top_words=5)\n","\n","#-------------------------- Part 1(c) ------------------------------------------------------------------------------#\n","\n","#Identifying unigram and bigram frequency of occurances\n","bigrams = ngrams(tokens, 2)\n","bigram_freq = dict(collections.Counter(bigrams))\n","unigrams = ngrams(tokens,1)\n","unigram_freq = dict(collections.Counter(unigrams))\n","\n","def probability(omega1,omega2):\n","    '''bigram probability P(omega2|omega1) '''\n","    numerator = None\n","    denominator = None\n","    #check if bigrams and unigrams exist in corpus\n","    try:\n","        numerator = bigram_freq[(omega1, omega2)]\n","    except:\n","        print(\"Bigram non-existant \")\n","        return None\n","    try:\n","        denominator = unigram_freq[(omega1,)]\n","    except:\n","        print(\"Unigram non-existant\")\n","        return None\n","\n","    return numerator/denominator\n","\n","print(\"\\n\\nExercise1 part1c\")\n","print(\"Built bigram... \")\n","n_samples = list(itertools.islice(bigram_freq.items(), 0, 3))\n","print(n_samples, \".... etc\\n\\nSorting the bigram freq table to observe the most occurancies\")\n","sorted_bigrams = sorted(bigram_freq.items(), key=lambda x: x[1], reverse=True)\n","print(\"\\nTop 3 most frequent bigrams:\")\n","print(sorted_bigrams[:3], \".... etc\")\n","print(\"\\n\\nObserving the probability of the pair 'in the' (as one of the top 3 occurance list), where the word 'in' occurs\",unigram_freq[('in',)])\n","print(probability(\"in\", \"the\"))\n","\n","#-------------------------- Part 1(D) ------------------------------------------------------------------------------#\n","\n","def part1d():\n","    '''gets the perplexity of the bigram model'''\n","    #Have to use Decimal object as if we use float, python rounds it to zero as the value of\n","    #total becomes very small as we multiply small probability values in the for loop\n","\n","    total = Decimal(1)\n","    for i in range(0, len(tokens)-2):\n","        total = total * Decimal(probability(tokens[i],tokens[i+1]))\n","\n","    perplexity = total ** Decimal(-1*(1/(len(tokens)-1)))\n","    print('Bigram  preplexity: ', perplexity)\n","\n","print(\"\\n\\nExercise1 part1d\")\n","part1d()\n","\n","\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"zzhff4ZAvHLU"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"apm","language":"python","name":"apm"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}